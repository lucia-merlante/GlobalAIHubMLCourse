{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBNr0n0-Ujl"
      },
      "source": [
        "#Homework 3\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQHTC5U2-bWM"
      },
      "source": [
        "Generate dataset using make_blobs function in the sklearn.datasets class.\r\n",
        "\r\n",
        "Generate 2000 samples with 3 features (X) with one label (y).\r\n",
        "\r\n",
        "Explore and analyse raw data.\r\n",
        "\r\n",
        "Do preprocessing for classification.\r\n",
        "\r\n",
        "Split your dataset into train and test test (0.7 for train and 0.3 for test).\r\n",
        "\r\n",
        "Try Decision Tree and XGBoost Algorithm with different hyperparameters. (Using GridSearchCV is a plus)\r\n",
        "\r\n",
        "Evaluate your result on both train and test set. Analyse if there is any underfitting or overfitting problem. Make your comments.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMHvlVZZ9EH"
      },
      "source": [
        "from sklearn.datasets import make_blobs\r\n",
        "X, y = make_blobs(n_samples=2000, centers=5, n_features=3, random_state=25)\r\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcX3_udeaSyH"
      },
      "source": [
        "import pandas as pd\r\n",
        "X = pd.DataFrame(X)\r\n",
        "y = pd.DataFrame(y)\r\n",
        "\r\n",
        "pd.concat([X, y], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y2l4X6AKHZF"
      },
      "source": [
        "##analyzing data\r\n",
        "\r\n",
        "As we can see, the 5 clusters are easily separable, even by human eye. I believe firmly the model can perform well on this generated dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUn9vZmpKJOz"
      },
      "source": [
        "#Scatter plot of train dataset\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "sns.scatterplot(X[0].T, X[1].T,color='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-7PT43fLrL7"
      },
      "source": [
        "#distribution of feature 1\r\n",
        "sns.distplot(X[0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZwIGBdLuDL"
      },
      "source": [
        "#distribution of feature 2\r\n",
        "sns.distplot(X[1].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmLomPULaCo"
      },
      "source": [
        "##splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M6om_yya3UN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYx_0qI_qOyu"
      },
      "source": [
        "##decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eBr4ZWJbOlA"
      },
      "source": [
        "from sklearn import tree\r\n",
        "\r\n",
        "clf = tree.DecisionTreeClassifier()\r\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QxNCMGibULr"
      },
      "source": [
        "tree.plot_tree(clf) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHcP7aqybZKu"
      },
      "source": [
        "#performance were very good\r\n",
        "clf.score(X_test, y_test)\r\n",
        "#0.9666"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxv-dagRc_0P"
      },
      "source": [
        "##xgboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvuAAQSKlRve"
      },
      "source": [
        "from xgboost.sklearn import XGBClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  \r\n",
        "\r\n",
        "param_dict = {\r\n",
        "    'max_depth':range(3,10,2),\r\n",
        "    'min_child_weight':range(1,6,2),\r\n",
        "    'learning_rate': [0.00001,0.001,0.01],\r\n",
        "    'n_estimators': [10,190,200,210,500]\r\n",
        "    \r\n",
        "}\r\n",
        "\r\n",
        "xgc = XGBClassifier(booster='gbtree', learning_rate =0.01, n_estimators=200, max_depth=5,\r\n",
        " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\r\n",
        " objective= 'multi:softprob', nthread=4, scale_pos_weight=1, seed=27)\r\n",
        "\r\n",
        "clf = GridSearchCV(xgc,param_dict,cv=3, n_jobs = -1).fit(X_train, y_train)\r\n",
        "\r\n",
        "print(\"Tuned: {}\".format(clf.best_params_)) \r\n",
        "print(\"Mean of the cv scores is {:.6f}\".format(clf.best_score_))\r\n",
        "print(\"Train Score {:.6f}\".format(clf.score(X_train,y_train)))\r\n",
        "print(\"Test Score {:.6f}\".format(clf.score(X_test,y_test)))\r\n",
        "print(\"Seconds used for refitting the best model on the train dataset: {:.6f}\".format(clf.refit_time_))\r\n",
        "\r\n",
        "#Tuned: {'learning_rate': 0.01, 'max_depth': 9, 'min_child_weight': 5, 'n_estimators': 500}\r\n",
        "#Mean of the cv scores is 0.970005\r\n",
        "#Train Score 0.987857\r\n",
        "#Test Score 0.980000\r\n",
        "#Seconds used for refitting the best model on the train dataset: 2.618238"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}